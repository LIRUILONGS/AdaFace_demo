---
title: AdaFace(Quality Adaptive Margin for Face Recognition)ï¼šé€šè¿‡AdaFaceå®ç°ä½è´¨é‡é¢éƒ¨æ•°æ®é›†çš„äººè„¸è¯†åˆ«
tags:
  - AdaFace
categories:
  - AdaFace
toc: true
recommend: 1
keywords: AdaFace
uniqueId: '2023-06-17 03:07:48/AdaFaceï¼š AdaFace(Quality Adaptive Margin for Face Recognition)ï¼šé€šè¿‡AdaFaceå®ç°ä½è´¨é‡é¢éƒ¨æ•°æ®é›†çš„äººè„¸è¯†åˆ«.html'
mathJax: false
date: 2023-06-17 11:07:48
thumbnail:
---

**<font color="009688"> å¯¹æ¯ä¸ªäººè€Œè¨€ï¼ŒçœŸæ­£çš„èŒè´£åªæœ‰ä¸€ä¸ªï¼šæ‰¾åˆ°è‡ªæˆ‘ã€‚ç„¶ååœ¨å¿ƒä¸­åšå®ˆå…¶ä¸€ç”Ÿï¼Œå…¨å¿ƒå…¨æ„ï¼Œæ°¸ä¸åœæ¯ã€‚æ‰€æœ‰å…¶å®ƒçš„è·¯éƒ½æ˜¯ä¸å®Œæ•´çš„ï¼Œæ˜¯äººçš„é€ƒé¿æ–¹å¼ï¼Œæ˜¯å¯¹å¤§ä¼—ç†æƒ³çš„æ‡¦å¼±å›å½’ï¼Œæ˜¯éšæ³¢é€æµï¼Œæ˜¯å¯¹å†…å¿ƒçš„ææƒ§ â€”â€”èµ«å°”æ›¼Â·é»‘å¡ã€Šå¾·ç±³å®‰ã€‹**</font>

<!-- more -->
## å†™åœ¨å‰é¢

***
+ å·¥ä½œä¸­é‡åˆ°ï¼Œç®€å•æ•´ç†
+ ä¸ªäººå¾ˆæ¨èè¿™ä¸ªæ¨¡å‹ï¼Œè¯†åˆ«ç›¸å¯¹è¦å¥½ä¸€ç‚¹
+ ç†è§£ä¸è¶³å°ä¼™ä¼´å¸®å¿™æŒ‡æ­£


**<font color="009688"> å¯¹æ¯ä¸ªäººè€Œè¨€ï¼ŒçœŸæ­£çš„èŒè´£åªæœ‰ä¸€ä¸ªï¼šæ‰¾åˆ°è‡ªæˆ‘ã€‚ç„¶ååœ¨å¿ƒä¸­åšå®ˆå…¶ä¸€ç”Ÿï¼Œå…¨å¿ƒå…¨æ„ï¼Œæ°¸ä¸åœæ¯ã€‚æ‰€æœ‰å…¶å®ƒçš„è·¯éƒ½æ˜¯ä¸å®Œæ•´çš„ï¼Œæ˜¯äººçš„é€ƒé¿æ–¹å¼ï¼Œæ˜¯å¯¹å¤§ä¼—ç†æƒ³çš„æ‡¦å¼±å›å½’ï¼Œæ˜¯éšæ³¢é€æµï¼Œæ˜¯å¯¹å†…å¿ƒçš„ææƒ§ â€”â€”èµ«å°”æ›¼Â·é»‘å¡ã€Šå¾·ç±³å®‰ã€‹**</font>

***

`ä½è´¨é‡äººè„¸æ•°æ®é›†`ä¸­çš„`è¯†åˆ«`å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºäººè„¸å±æ€§è¢«æ¨¡ç³Šå’Œé™çº§ã€‚åŸºäº`è£•é‡çš„æŸå¤±å‡½æ•°`çš„è¿›æ­¥æé«˜äº†åµŒå…¥ç©ºé—´ä¸­äººè„¸çš„å¯è¾¨åˆ«æ€§ã€‚


æ­¤å¤–ï¼Œä»¥å‰çš„ç ”ç©¶å·²ç»ç ”ç©¶äº†`é€‚åº”æ€§æŸå¤±`çš„å½±å“ï¼Œä»¥æ›´åŠ é‡è§†`é”™è¯¯åˆ†ç±»`çš„ï¼ˆç¡¬ï¼‰ä¾‹å­ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†`æŸå¤±å‡½æ•°è‡ªé€‚åº”æ€§`çš„å¦ä¸€ä¸ªæ–¹é¢ï¼Œå³`å›¾åƒè´¨é‡`ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œå¼ºè°ƒé”™è¯¯åˆ†ç±»æ ·æœ¬çš„ç­–ç•¥åº”æ ¹æ®å…¶å›¾åƒè´¨é‡è¿›è¡Œè°ƒæ•´ã€‚å…·ä½“æ¥è¯´ï¼Œç®€å•å’Œç¡¬æ ·å“çš„ç›¸å¯¹é‡è¦æ€§åº”åŸºäºæ ·å“çš„å›¾åƒè´¨é‡ã€‚

æˆ‘ä»¬`æå‡ºäº†ä¸€ç§æ–°çš„æŸå¤±å‡½æ•°ï¼Œè¯¥å‡½æ•°æ ¹æ®å›¾åƒè´¨é‡å¼ºè°ƒä¸åŒéš¾åº¦çš„æ ·æœ¬ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ç”¨ç‰¹å¾èŒƒæ•°è¿‘ä¼¼å›¾åƒè´¨é‡ï¼Œä»¥è‡ªé€‚åº”è£•é‡å‡½æ•°çš„å½¢å¼å®ç°è¿™ä¸€ç‚¹`ã€‚å¤§é‡çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•`AdaFace`åœ¨å››ä¸ªæ•°æ®é›†ï¼ˆIJB-Bï¼ŒIJB-Cï¼ŒIJB-Så’ŒTinyFaceï¼‰ä¸Šæé«˜äº†æœ€å…ˆè¿›çš„ï¼ˆSoTAï¼‰çš„äººè„¸è¯†åˆ«æ€§èƒ½ã€‚


```bash
@inproceedings{kim2022adaface,
  title={AdaFace: Quality Adaptive Margin for Face Recognition},
  author={Kim, Minchul and Jain, Anil K and Liu, Xiaoming},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2022}
}
```

å®é™…æµ‹è¯•ä¸­å‘ç°ï¼Œ`AdaFace` ç¡®å®å¾ˆå¼ºå¤§ï¼Œç‰¹åˆ«é€‚åˆè¿œè·ç¦»ï¼Œå°ç›®æ ‡ï¼Œå›¾ç‰‡è´¨é‡ä½çš„äººè„¸è¯†åˆ«ã€‚


[https://github.com/mk-minchul/AdaFace](https://github.com/mk-minchul/AdaFace)

å…‹éš†é¡¹ç›®ï¼Œç¯å¢ƒæ­å»º

```bash
(base) C:\Users\liruilong>conda create -n AdaFace python==3.8
Solving environment: done
(base) C:\Users\liruilong>conda activate AdaFace

(AdaFace) C:\Users\liruilong>cd Documents\GitHub\AdaFace_demo

(AdaFace) C:\Users\liruilong\Documents\GitHub\AdaFace_demo>conda install scikit-image matplotlib pandas scikit-learn
Solving environment: done
ã€‚ã€‚ã€‚
(AdaFace) C:\Users\liruilong\Documents\GitHub\AdaFace_demo>pip install -r requirements.txt  -i http://pypi.douban.com/simple/ --trusted-host pypi.douban.com
Looking in indexes: http://pypi.douban.com/simple/
```

æ²¡æœ‰GPUï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦ä¿®æ”¹åŸä»£ç ä¸­éƒ¨åˆ†ä¸º CPU å¯ä»¥æ‰§è¡Œ

ä¿®æ”¹ä½ç½®ï¼š`\GitHub\AdaFace_demo\face_alignment\align.py`
```bash
mtcnn_model = mtcnn.MTCNN(device='cuda:0', crop_size=(112, 112))
# ä¿®æ”¹ä¸º
mtcnn_model = mtcnn.MTCNN(device='cpu', crop_size=(112, 112))
```
ä¿®æ”¹ä½ç½®ï¼š`\GitHub\AdaFace_demo\inference.py`

```bash
statedict = torch.load(adaface_models[architecture])['state_dict']
# ä¿®æ”¹ä¸º
statedict = torch.load(adaface_models[architecture], map_location=torch.device('cpu'))['state_dict']
```



ä¹‹åéœ€è¦ä¸‹è½½å¯¹åº”çš„æ¨¡å‹æ–‡ä»¶ï¼Œå¯ä»¥åš github çœ‹åˆ°ã€‚æ”¾åˆ°æŒ‡å®šä½ç½®`pretrained/adaface_ir101_webface12m.ckpt`å°±å¯ä»¥æ‰§è¡Œäº†ï¼Œè¿™é‡Œä¸å¤šè®²


è¿è¡Œ Demo,3 å¼ å›¾ç‰‡æ¯”è¾ƒ
```bash
(AdaFace) C:\Users\liruilong\Documents\GitHub\AdaFace_demo> python inference.py
C:\Users\liruilong\Documents\GitHub\AdaFace_demo\face_alignment\mtcnn_pytorch\src\matlab_cp2tform.py:90: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.
To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.
  r, _, _, _ = lstsq(X, U)
inference.py:25: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\torch\csrc\utils\tensor_new.cpp:248.)
  tensor = torch.tensor([brg_img.transpose(2,0,1)]).float()
tensor([[ 1.0000,  0.7329, -0.0794],
        [ 0.7329,  1.0000, -0.0087],
        [-0.0794, -0.0087,  1.0000]], grad_fn=<MmBackward0>)
```

è¿™é‡Œçš„çŸ©é˜µè¡¨ç¤ºï¼Œæ¯å¼ å›¾ç‰‡ç›¸äº’æ¯”è¾ƒï¼ŒçŸ©é˜µä¸º3*3ï¼Œä¸‰è¡Œä¸‰åˆ—ï¼Œç¬¬ä¸€å¼ å›¾ç‰‡è·Ÿç¬¬ä¸€å¼ å›¾ç‰‡çš„ç›¸ä¼¼åº¦ä¸º 1 (è‡ªå·±å’Œè‡ªå·±æ¯”)ï¼Œç„¶åç¬¬ä¸€å¼ å›¾ç‰‡è·Ÿç¬¬äºŒå¼ å›¾ç‰‡å¯¹æ¯”çš„ç›¸ä¼¼åº¦ä¸º ` 0.7329`ï¼Œç¬¬ä¸€å¼ å›¾ç‰‡è·Ÿç¬¬ä¸‰å¼ å›¾ç‰‡å¯¹æ¯”çš„ç›¸ä¼¼åº¦ä¸º `-0.0794`ï¼Œå¯¹è§’éƒ½ä¸ºè‡ªå·±å’Œè‡ªå·±æ¯”è¾ƒæ‰€ä»¥æ˜¯1.

æˆ‘ä»¬é€šè¿‡ä¸Šé¢ä½™å¼¦ç›¸ä¼¼åº¦å¾—åˆ†å¯ä»¥åŒºåˆ†æ˜¯å¦æ˜¯ä¸€ä¸ªäººï¼Œåœ¨å…·ä½“çš„äººè„¸è¯†åˆ«åº”ç”¨ä¸­ã€‚

1. éœ€è¦é¢„å…ˆé€šè¿‡ä¸Šé¢çš„æ¨¡å‹æŠŠäººè„¸åº“æ¯å¼ ç…§ç‰‡çš„ç‰¹å¾å‘é‡ä¿å­˜åˆ°æ–‡æœ¬é‡Œï¼Œç”Ÿæˆç‰¹å¾å‘é‡é›† `Ax={A1,A2,A3,....Ai}(i=n)`
2. éœ€è¦è¯†åˆ«çš„æ—¶å€™ï¼Œé€šè¿‡æ¨¡å‹è·å–è¦è¯†åˆ«ç…§ç‰‡çš„ç‰¹å¾å‘é‡ `B1`ï¼Œç”¨ç‰¹å¾å‘é‡é›† `Ax` ä¸­çš„æ¯ä¸ªå‘é‡`Ai`å’Œè¯†åˆ«ç…§ç‰‡çš„ç‰¹å¾å‘é‡`B1`è·å–ä½™å¼¦ç›¸ä¼¼åº¦å¾—åˆ†
3. å¯¹äºè·å–çš„ç›¸ä¼¼å¾—åˆ†æœ€å¤§å€¼ï¼Œå’Œé˜ˆå€¼åˆ¤æ–­ï¼Œæˆ–è€…å–å¤§äºé˜ˆå€¼å†…çš„æ•°æ®ï¼Œåˆ¤æ–­æ˜¯å¦ä¸ºä¸€ä¸ªäººï¼Œä»è€Œå®ç°äººè„¸è¯†åˆ«ã€‚
4. ç›¸ä¼¼åº¦å¾—åˆ†æ˜¯ä¸€ä¸ªæ¥è¿‘ å°äºç­‰ä¸€ï¼Œå¤§äºç­‰äº -1 çš„å€¼ï¼Œè¶Šå¤§ç›¸è¯†åº¦è¶Šé«˜ï¼Œç­‰äº1 å³æ˜¯ç¡®å®šåŒä¸€ä¸ªäººï¼Œ-1 å³å®Œå…¨ä¸æ˜¯ä¸€ä¸ªäººï¼Œ å®é™…è¯†åˆ«ä¸­ï¼Œç»™ç›¸ä¼¼å¾—åˆ†ä¸€ä¸ªé˜ˆå€¼ï¼Œåœ¨è¿™ä¸ªèŒƒå›´å†…æˆ‘ä»¬ç¡®å®šä¸ºä¸€ä¸ªäººã€‚

ä¸‹é¢ä¸ºä¸€ä¸ª Demo

+ `build_vector_pkl` ç”¨äºç”Ÿæˆç‰¹å¾æ–‡ä»¶(éœ€è¦å‡†å¤‡ä¸€ä¸ªç…§ç‰‡æ•°æ®é›†ï¼Œé€šè¿‡ç…§ç‰‡åå­—æ ‡æ³¨äºº)
+ `read_vector_pkl` ç”¨äºåŠ è½½ç‰¹å¾æ–‡ä»¶åˆ°å†…å­˜
+ `find` ç”¨äºéœ€è¦è¯†åˆ«çš„äººè„¸å’Œäººè„¸åº“æ¯”å¯¹ï¼Œè¿”å›è¯†åˆ«ç»“æœ
+ è¿™é‡Œé»˜è®¤æˆ‘ä»¬å·²ç»é€šè¿‡æ£€æµ‹ï¼Œæ¯å¼ è¯†åˆ«ç…§ç‰‡åªæœ‰ä¸€ä¸ªäººè„¸ã€‚

```py
import net
import torch
import os
from face_alignment import align
import numpy as np
import pandas as pd

adaface_models = {'ir_101': "pretrained/adaface_ir101_webface12m.ckpt",}

def load_pretrained_model(architecture='ir_101'):
    # load model and pretrained statedict
    assert architecture in adaface_models.keys()
    model = net.build_model(architecture)
    statedict = torch.load(
        adaface_models[architecture], map_location=torch.device('cpu'))['state_dict']
    model_statedict = {key[6:]: val for key,
                       val in statedict.items() if key.startswith('model.')}
    model.load_state_dict(model_statedict)
    model.eval()
    return model

def to_input(pil_rgb_image):
    tensor = None
    try:
        np_img = np.array(pil_rgb_image)
        brg_img = ((np_img[:, :, ::-1] / 255.) - 0.5) / 0.5
        tensor = torch.tensor([brg_img.transpose(2, 0,1)]).float()
    except Exception :
        return tensor    
    return tensor

def read_vector_pkl(db_path, adaface_model_name):
    """
    @Time    :   2023/06/16 12:10:47
    @Author  :   liruilonger@gmail.com
    @Version :   1.0
    @Desc    :   è¯»å–ç‰¹å¾å‘é‡æ–‡ä»¶
                 Args:
                   
                 Returns:
                   df
    """
    import pickle
    file_name = f"representations_adaface_{adaface_model_name}.pkl"
    file_name = file_name.replace("-", "_").lower()
    with open(f"{db_path}/{file_name}", "rb") as f:
            representations = pickle.load(f)
    df = pd.DataFrame(representations, columns=["identity", f"{adaface_model_name}_representation"])
    return df


def build_vector_pkl(db_path, adaface_model_name='adaface_model'):
    """
    @Time    :   2023/06/16 11:40:23
    @Author  :   liruilonger@gmail.com
    @Version :   1.0
    @Desc    :   æ„å»ºç‰¹å¾å‘é‡æ–‡ä»¶
                 Args:

                 Returns:
                   void
    """
    import time
    from os import path
    from tqdm import tqdm
    import pickle

    if os.path.isdir(db_path) is not True:
        raise ValueError("Passed db_path does not exist!")

    file_name = f"representations_adaface_{adaface_model_name}.pkl"
    file_name = file_name.replace("-", "_").lower()
    if path.exists(db_path + "/" + file_name):
        pass
    else:
        employees = []
        for r, _, f in os.walk(db_path):
            for file in f:
                if (
                    (".jpg" in file.lower())
                    or (".jpeg" in file.lower())
                    or (".png" in file.lower())
                ):
                    exact_path = r + "/" + file
                    employees.append(exact_path)

        if len(employees) == 0:
            raise ValueError(
                "æ²¡æœ‰ä»»ä½•å›¾åƒåœ¨  ",
                db_path,
                "  æ–‡ä»¶å¤¹! éªŒè¯æ­¤è·¯å¾„ä¸­æ˜¯å¦å­˜åœ¨.jpgæˆ–.pngæ–‡ä»¶ã€‚",
            )
        representations = []

        pbar = tqdm(
            range(0, len(employees)),
            desc="ç”Ÿæˆå‘é‡ç‰¹å¾æ–‡ä»¶ä¸­ï¼šğŸ˜ğŸ˜ŠğŸ”¬ğŸ”¬ğŸ”¬âš’ï¸âš’ï¸âš’ï¸ğŸ¢ğŸ¢ğŸ¢ğŸ¢ğŸ¢",
        )
        for index in pbar:
            employee = employees[index]

            img_representation = get_represent(employee)
            instance = []
            instance.append(employee)
            instance.append(img_representation)
            representations.append(instance)
        with open(f"{db_path}/{file_name}", "wb") as f:
            pickle.dump(representations, f)


def get_represent(path):
    """
    @Time    :   2023/06/16 11:54:11
    @Author  :   liruilonger@gmail.com
    @Version :   1.0
    @Desc    :   è·å–è„¸éƒ¨ç‰¹å¾å‘é‡
                 Args:
                   
                 Returns:
                   void
    """
    feature = None
    aligned_rgb_img = align.get_aligned_face(path)
    bgr_tensor_input = to_input(aligned_rgb_img)
    if bgr_tensor_input is not None:
        feature, _ = model(bgr_tensor_input)
    else:
       print(f"æ— æ³•æå–è„¸éƒ¨ç‰¹å¾å‘é‡{path}")     
    return feature

def findCosineDistance(source_representation, test_representation):
    """
    @Time    :   2023/06/16 12:19:27
    @Author  :   liruilonger@gmail.com
    @Version :   1.0
    @Desc    :   è®¡ç®—ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦å¾—åˆ†
                 Args:
                   
                 Returns:
                   void
    """
    import torch.nn.functional as F
    return F.cosine_similarity(source_representation, test_representation)


def demo1():
    model_name = "test"
    build_vector_pkl(test_image_path,model_name)
    df = read_vector_pkl(test_image_path, model_name)
    for index, instance in df.iterrows():
        source_representation = instance[f"{model_name}_representation"]
        #distance = findCosineDistance(source_representation, target_representation)
        print(source_representation)
        features.append(source_representation)
    similarity_scores = torch.cat(features) @ torch.cat(features).T   
    print(similarity_scores)


def find(test_image_path,threshold=0.5):
    """
    @Time    :   2023/06/16 14:02:52
    @Author  :   liruilonger@gmail.com
    @Version :   1.0
    @Desc    :   æ ¹æ®å›¾ç‰‡åœ¨äººè„¸åº“æ¯”å¯¹æ‰¾äºº
                 Args:
                   
                 Returns:
                   void
    """
        
    test_representation = get_represent(test_image_path)
    if test_representation  is  not None:
        reset = {}
        for index, instance in df.iterrows():
            source_representation = instance[f"{model_name}_representation"]
            ten = findCosineDistance(source_representation,test_representation)
            reset[ten.item()]= instance["identity"]        
        cosine_similarity =  max(reset.keys())        
        print(f"ğŸ’ğŸ’ğŸ’ğŸ’ğŸ’ğŸ’ğŸ’ğŸ’ğŸ’ğŸ’ {cosine_similarity} ğŸ’ğŸ’ğŸ’ğŸ’ğŸ’{threshold}")
        return cosine_similarity > threshold ,reset[cosine_similarity]
    else:
        return False,0

def marge(m1,m2):
    from PIL import Image
    import uuid
    # æ‰“å¼€ç¬¬ä¸€å¼ å›¾ç‰‡
    image1 = Image.open(m1)
    # æ‰“å¼€ç¬¬äºŒå¼ å›¾ç‰‡
    image2 = Image.open(m2)
    # è·å–ç¬¬ä¸€å¼ å›¾ç‰‡çš„å¤§å°
    width1, height1 = image1.size
    # è·å–ç¬¬äºŒå¼ å›¾ç‰‡çš„å¤§å°
    width2, height2 = image2.size
    # åˆ›å»ºä¸€ä¸ªæ–°çš„ç”»å¸ƒï¼Œå¤§å°ä¸ºä¸¤å¼ å›¾ç‰‡çš„å®½åº¦ä¹‹å’Œå’Œé«˜åº¦çš„æœ€å¤§å€¼
    new_image = Image.new("RGB", (width1 + width2, max(height1, height2)))
    # å°†ç¬¬ä¸€å¼ å›¾ç‰‡ç²˜è´´åˆ°ç”»å¸ƒçš„å·¦ä¾§
    new_image.paste(image1, (0, 0))
    # å°†ç¬¬äºŒå¼ å›¾ç‰‡ç²˜è´´åˆ°ç”»å¸ƒçš„å³ä¾§
    new_image.paste(image2, (width1, 0))
    # ä¿å­˜æ‹¼æ¥åçš„å›¾ç‰‡
    new_image.save(str(uuid.uuid4()).replace('-', '')+"new_image.jpg")



if __name__ == '__main__':
   
    import imutils 
    from imutils import paths
    import cv2
    import uuid
    model = load_pretrained_model('ir_101')
    # éœ€è¦è¯†åˆ«çš„å›¾ç‰‡ä½ç½®
    test_image_path = 'face_alignment/ser'
    features = set()
    model_name = "test_img"

    build_vector_pkl("face_alignment/test",model_name)
    df = read_vector_pkl("face_alignment/test", model_name)
    
    for path in paths.list_images(test_image_path):
        b, r = find(path,0.25)
        if b:
            if r not in features:
                features.add(r)
                marge(r,path)
        else:
            img = cv2.imread(path)
            cv2.imwrite('__not'  + str(uuid.uuid4()).replace('-', '')+".jpg", img)
        
```


## åšæ–‡éƒ¨åˆ†å†…å®¹å‚è€ƒ

Â© æ–‡ä¸­æ¶‰åŠå‚è€ƒé“¾æ¥å†…å®¹ç‰ˆæƒå½’åŸä½œè€…æ‰€æœ‰ï¼Œå¦‚æœ‰ä¾µæƒè¯·å‘ŠçŸ¥ï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºé¡¹ç›®ï¼Œå¦‚æœä½ è®¤å¯å®ƒï¼Œä¸è¦åå•¬æ˜Ÿæ˜Ÿå“¦ :)


***

AdaFace: Quality Adaptive Margin for Face Recognition(AdaFaceï¼šç”¨äºäººè„¸è¯†åˆ«çš„è´¨é‡è‡ªé€‚åº”è£•é‡): [https://arxiv.org/abs/2204.00964](https://arxiv.org/abs/2204.00964)

https://github.com/mk-minchul/AdaFace

***

Â© 2018-2023 liruilonger@gmail.com, All rights reserved. ä¿æŒç½²å-éå•†ç”¨-ç›¸åŒæ–¹å¼å…±äº«(CC BY-NC-SA 4.0)
